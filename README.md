# TextualEntailmentBiLSTMAttention
A Bi-Directional LSTM with Neural Attention and word embeddings. Tackles the difficult problem of Textual Entailment using the Stanford Natural Language Inference (SNLI) corpus. Demonstrates that a 3-class validation accuracy of 76%+ can be obtained on the corpus without resorting to pre-training or recursion/trees. Concept pioneered in "Reasoning about Entailment with Neural Attention" by Rockt√§schel et al. Inspiration taken from https://github.com/shyamupa/snli-entailment. Please find data corpus at https://nlp.stanford.edu/projects/snli/

<p align="center">
  <img src="https://github.com/michaelznidarsic/TextualEntailmentBiLSTMAttention/blob/master/NN%20Full%20Test%20CM.png" />
</p>


<p align="center">
  <img src="https://github.com/michaelznidarsic/TextualEntailmentBiLSTMAttention/blob/master/multichannel2.png" />
</p>


